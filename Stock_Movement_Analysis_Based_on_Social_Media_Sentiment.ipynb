{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape data from Reddit for stock market discussions using PRAW"
      ],
      "metadata": {
        "id": "MGNqTRnON9Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up PRAW"
      ],
      "metadata": {
        "id": "MWRHWePlNkfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw"
      ],
      "metadata": {
        "id": "CNWSYo5n_-DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Reddit Instance"
      ],
      "metadata": {
        "id": "1aOd5-k2NwhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"your_client_id\",\n",
        "    client_secret=\"your_client_secret\",\n",
        "    username=\"your_reddit_username\",\n",
        "    password=\"your_reddit_password\",\n",
        "    user_agent=\"your_user_agent\",\n",
        ")"
      ],
      "metadata": {
        "id": "aEHrRt9-OZVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the subreddits you want to scrape data from, such as \"wallstreetbets\", \"stocks\", \"investing\", and \"StockMarket\"."
      ],
      "metadata": {
        "id": "9bAdkOrROcrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddits_to_parse = ['wallstreetbets', 'stocks', 'investing', 'StockMarket']"
      ],
      "metadata": {
        "id": "nNomaH4WOh0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "#extract_data\n",
        "for sub in subreddits_to_parse:\n",
        "    subreddit_instance = reddit.subreddit(sub)\n",
        "    submissions = subreddit_instance.hot(limit=50)  # get_the_top_50_hot_posts\n",
        "    for submission in submissions:\n",
        "        print(f\"Submission title: {submission.title}\")\n",
        "#analysing_the_comment\n",
        "for submission in submissions:\n",
        "    submission.comments.replace_more(limit=0)  # flatten_the_comment_tree\n",
        "    comments = submission.comments.list()  # get_all_comments\n",
        "    for comment in comments:\n",
        "        print(f\"Comment: {comment.body}\")\n",
        "#filter_stocks_tickets\n",
        "def clean_word(word):\n",
        "    return re.sub(r'[^\\w\\s]', '', word).upper()  # remove_punctuation_and_convert_to_uppercase\n",
        "\n",
        "potential_stock_symbols = []\n",
        "for word in word_collection:\n",
        "    cleaned_word = clean_word(word)\n",
        "    if cleaned_word.isupper() and not containsNumber(cleaned_word):\n",
        "        potential_stock_symbols.append(cleaned_word)\n",
        "#count_frequency\n",
        "cnt = Counter(potential_stock_symbols)\n",
        "trending_tickers = [k for k, v in cnt.items() if v > 50]  # filter_tickers_mentioned_more_than_50_times\n",
        "print(\"Trending Stock Tickers:\", trending_tickers)"
      ],
      "metadata": {
        "id": "kVnINkyCPBSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Scraping from Reddit"
      ],
      "metadata": {
        "id": "FPZNkPl8OlLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize a list to hold the data\n",
        "data = []\n",
        "\n",
        "#loop_through_extract_data_post_comment_and_subreddits\n",
        "for sub in subreddits_to_parse:\n",
        "    subreddit_instance = reddit.subreddit(sub)\n",
        "    submissions = subreddit_instance.hot(limit=50)  # get_the_top_50_hot_posts\n",
        "    for submission in submissions:\n",
        "        submission.comments.replace_more(limit=0)  # flatten_the_comment_tree\n",
        "        comments = submission.comments.list()  # get_all_comments\n",
        "        for comment in comments:\n",
        "            data.append({\n",
        "                'subreddit': sub,\n",
        "                'title': submission.title,\n",
        "                'comment': comment.body,\n",
        "                'created_utc': submission.created_utc\n",
        "            })\n",
        "\n",
        "# create_a_dataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('reddit_stock_data.csv', index=False)"
      ],
      "metadata": {
        "id": "-IH0DKhWQT2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "qHAm2qE5RTGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load_the_data\n",
        "df = pd.read_csv('reddit_stock_data.csv')\n",
        "\n",
        "# remove_duplicates_and_nulls\n",
        "df.drop_duplicates(subset=['title', 'comment'], inplace=True)\n",
        "df.dropna(subset=['comment'], inplace=True)\n",
        "\n",
        "# further_cleaning\n",
        "df['comment'] = df['comment'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False)\n",
        "df['comment'] = df['comment'].str.replace(r'\\@\\w+|\\#', '', '', case=False)"
      ],
      "metadata": {
        "id": "P-P0MK5BRSmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Sentiment Analysis"
      ],
      "metadata": {
        "id": "JrvX5WU0RnQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# download_VADER_lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# initialize_the_VADER_sentiment_analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# apply_sentiment_analysis\n",
        "df['sentiment'] = df['comment'].apply(lambda x: sia.polarity_scores(x)['compound'])"
      ],
      "metadata": {
        "id": "P0a0-3H0Ro5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Feature Extraction"
      ],
      "metadata": {
        "id": "3xDwwcTpR_vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['length'] = df['comment'].apply(len)  # length_of_the_comment\n",
        "df['is_positive'] = df['sentiment'] > 0  # binary_feature_for_positive_sentiment\n",
        "\n",
        "# grp_by_date_and_aggregate_features\n",
        "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
        "df.set_index('created_utc', inplace=True)\n",
        "\n",
        "# resample_to_daily_frequency_and_aggregate\n",
        "daily_data = df.resample('D').agg({\n",
        "    'sentiment': 'mean',\n",
        "    'length': 'mean',\n",
        "    'is_positive': 'sum'\n",
        "}).fillna(0)\n",
        "\n",
        "# save_the_processed_data\n",
        "daily_data.to_csv('daily_stock_sentiment.csv')"
      ],
      "metadata": {
        "id": "qmVcJWj4SHht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Prediction Model"
      ],
      "metadata": {
        "id": "VZa-4z8JXMfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# required_libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "daily_data['price_movement'] = np.random.choice([0, 1], size=len(daily_data))  # Binary target: 0 = Down, 1 = Up\n",
        "\n",
        "# Features and Target\n",
        "X = daily_data[['sentiment', 'length', 'is_positive']]\n",
        "y = daily_data['price_movement']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Comparison\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Support Vector Machine': SVC(kernel='linear', random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    # Append results\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1\n",
        "    })\n",
        "\n",
        "# Create a DataFrame for results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(\"Model Comparison:\")\n",
        "print(results_df)\n",
        "\n",
        "# Choose the best model\n",
        "best_model_name = results_df.sort_values('F1 Score', ascending=False).iloc[0]['Model']\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "\n",
        "# Plot comparison\n",
        "results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Model Comparison Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MClXeIIKXQvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}